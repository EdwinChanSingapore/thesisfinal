\relax 
\pgfsyspdfmark {pgfid1}{5738987}{49989344}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Next Generation Sequencing (NGS) for Clinical Genomics}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Variant Calling of NGS Data}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Variant Calling Pileup - Due to noise and errors in sequencing and read mapping, it can be difficult to accurately call variants. Figure adapted from CLC Genomics Workbench 9.5, Figure 29.8.\relax }}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Performance of Variant Calling tools on Ratient Data using Different Illumina Sequencing Platforms (HiSeq and NextSeq). The F1 score indicates how well a caller can predict true positives (See Appendix 5.3 for more details). Notably the F1 score for the same variant calling tool can differ greatly. Figure adapted from Sandmann et al. (2017)\relax }}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Ensemble Methods for Improving the Accuracy of Variant Calling}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Deep Learning for Improving the Accuracy of Variant Calling}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Artificial Neurons as Building Blocks of Neural Networks\relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A Neural Network with one input layer, three hidden layers and one output layer. This represents a densely connected neural network, where each node is connected to every node of the preceding and subsequent layers.\relax }}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Prioritisation of Variants with Bayesian Networks }{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A Sample Bayesian Network for Rain Prediction.\relax }}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Aims and Approach}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Materials and Methods}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Overall Analysis Structure}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Overall Analytical Pipelines -- Pipelines were implemented using the Groovy Domain Specific Language, NextFlow.\relax }}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Programming and Pipelining tools}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}General Programming Language}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Pipelining}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Deep Learning}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Gene Ranking}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Artificial Datasets}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Pipeline for simulation of artificial genome for analysis\relax }}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Alignment and Variant Calling}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Feature Engineering}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Patient Derived Xenograft Mouse Model Development and Sequencing}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Generation of Artificial Datasets}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Number of ground truth mutations (variants) created in each chromosome \relax }}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Mutation rate per base in each chromosome\relax }}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Feature Engineering}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Feature Engineering Table\relax }}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Variant Callers}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Concordance of callers on simulated dataset, using default settings\relax }}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Table Comparing Methods and Features of Different variant callers.\relax }}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Network Architecture}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Different Designs for Neural Network Architecture\relax }}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Analysis of Different Neural Network Architecture\relax }}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Network Tuning and Optimisation}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Number of Layers}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Basic Merge Network Structure\relax }}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Analysis of Different Number of Layers On Training Accuracy\relax }}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Optimiser and Learning Rates}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Optimiser accuracies for training at each epoch. Due to the noise in accuracies, the overall momentum of the dataset, calculated as a sliding window average is shown. The 95\% confidence interval is also shown.\relax }}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Training Accuracies over Each Epoch for Different Learning Rates\relax }}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Sample Balancing}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces \textbf  {a)} Graphical Illustration of Sample Balancing \textbf  {b)} Effect of Sample Balancing Techniques on Prediction Ability\relax }}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Benchmarking of Optimised Network with Mason Dataset}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Overall Comparison of Variant Callers\relax }}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Comparison of Best Variant Callers in terms of Precision, Recall and F1 Score\relax }}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Benchmarking of Optimised Network with NA Dataset}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Comparison of Variant Callers\relax }}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Analysis of Gene Importance using Bayesian Ranking systems}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Final Bayesian Network used in Analysis\relax }}{29}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Table of Functional Annotations obtained from ANNOVAR\relax }}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9}Validation of Bayesian Network Ranking on PDX dataset}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Top 30 genes from Bayesian Ranking Algorithm\relax }}{31}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Table of Highest Ranked Genes from Bayesian Ranking\relax }}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces 5 year survival curve of patients with SPRR1A+ and SPRR1A- patients with DLBCL. Source : Zhang et al. (2014), Figure 2.\relax }}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Circos plot of top 300 ranked genes from Bayesian network ranking. In this Circos plot, the outer track indicates the top ranked genes and their positions on the chromosome. The inner track describes the type of mutation that was observed -- most mutations were non-synonymous SNVs, with a few stop-gain mutations. The innermost track shows the relative probabilities of each ranked gene.\relax }}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Comparison of Deep Learning with other Integration Methods}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Analysis of Bayesian Network}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Future Directions}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Appendixes}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Neural Network Learning}{37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Feedforward Phase}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Example neural networks with nodes and weights\relax }}{37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Backpropagation Phase}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Backpropagation of Error Terms\relax }}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Cost Function and Backpropagation}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Gradient Descent, which attempts to find the gradient at which the cost function is minimised (since the cost function depends on the gradient).\relax }}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Feature Engineering}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Base Information}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Sequencing Biases and Errors}{41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Calling and Mapping Qualities}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Mathematical and Statistical Tools}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Derivation of F1 Score}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Confusion Matrix\relax }}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Principal Components Analysis (PCA)}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Variance captured by first 12 principal components\relax }}{44}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Synthetic Minority Overrepresentation Technique (SMOTE)}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces SMOTE oversampling algorithm\relax }}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Bibilography}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Relevant Code}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}generate\_matrixes.py}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}train\_network.py}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}compute\_bayesian.py}{69}}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  708C9BCB12B5216BBF86DAFDD16ECE411783A10277C0CC29BFFEC5BB6F4B264B.pygtex,
  B85D79633CB32D38A6E77E93497D5E651783A10277C0CC29BFFEC5BB6F4B264B.pygtex,
  4997910BF66F97E9C5DC9311827F542B1783A10277C0CC29BFFEC5BB6F4B264B.pygtex}
