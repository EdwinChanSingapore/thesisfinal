\relax 
\pgfsyspdfmark {pgfid1}{7613811}{48094858}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Next Generation Sequencing (NGS) for Clinical Genomics}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Variant Calling of NGS Data}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \textbf  {Illustration of Variant Calling Pileup}. Due to noise and errors in sequencing and read mapping, it can be difficult to accurately call variants. At the position of interest, there seems to be a possible mutation from the original base of Cytosine to the base Adenine, but not all reads agree with this mutation call. Figure adapted from CLC Genomics Workbench 9.5, Figure 29.8.\relax }}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Performance of Variant Calling Tools on Patient Data using Illumina Sequencing Platforms HiSeq and NextSeq}. The F1 score indicates how well a caller can predict true positives (See Appendix 5.3 for more details). Notably the F1 score for the same variant calling tool can differ greatly. Figure adapted from Sandmann et al. (2017)\relax }}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Ensemble Methods for Improving the Accuracy of Variant Calling}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Deep Learning for Improving the Accuracy of Variant Calling}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Artificial Neurons as Building Blocks of Neural Networks}\relax }}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Sample Neural Network with Five Layers, Including Three Hidden Layers}. This represents a densely connected neural network, where each node is connected to every node of the preceding and subsequent layers.\relax }}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Prioritisation of Variants with Bayesian Networks }{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Sample Bayesian Network for Rain Prediction.}\relax }}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Aims and Approach}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Materials and Methods}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Overall Analysis Structure}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Overall Structure of Analytical Pipelines.} Pipelines were implemented using the Groovy Domain Specific Language, NextFlow.\relax }}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Programming and Pipelining tools}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}General Programming Language}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Pipelining}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Deep Learning}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Gene Ranking}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Artificial Datasets}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Pipeline for Simulation of Artificial Genomes for Analysis}\relax }}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Alignment and Variant Calling}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Feature Engineering}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Patient Derived Xenograft Mouse Model Development and Sequencing}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Generation of Artificial Datasets}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Number of Ground Truth Mutations (Variants) Created in Each Chromosome}\relax }}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Mutation Rate per Base in Each Chromosome}\relax }}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Feature Engineering}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Feature Engineering Table}\relax }}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Variant Callers}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Concordance of Callers on Simulated Dataset}\relax }}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Comparison of Different Methods and Features of Variant Callers.}\relax }}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Network Architecture}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Different Designs for Neural Network Architecture}\relax }}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {Analysis of Different Neural Network Architecture}\relax }}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Network Tuning and Optimisation}{23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Number of Layers}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {Basic Merge Network Structure.} Each individual pre-merge layer takes in an input from a single caller. The information is then integrated in a set of merge layers to give a prediction.\relax }}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \textbf  {Analysis of Different Number of Layers On Training Accuracy}\relax }}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Optimiser and Learning Rates}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \textbf  {Optimiser Accuracies for Training at each Epoch.} Due to the noise in accuracies, the overall momentum of the dataset, calculated as a sliding window average is shown. The 95\% confidence interval is also shown.\relax }}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces \textbf  {Training Accuracies over Each Epoch for Different Learning Rates}\relax }}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Sample Balancing}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces \textbf  {a)} \textbf  {Graphical Illustration of Sample Balancing.} \textbf  {b) Effect of Sample Balancing Techniques on Prediction Ability.}\relax }}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Benchmarking of Optimised Network with Mason Dataset}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces \textbf  {Overall Comparison of Variant Callers}\relax }}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces \textbf  {Comparison of Best Variant Callers in terms of Precision, Recall and F1 Score}\relax }}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Benchmarking of Optimised Network with NA Dataset}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces \textbf  {Comparison of Variant Callers}\relax }}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Analysis of Gene Importance using Bayesian Ranking systems}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces \textbf  {Final Bayesian Network used in Analysis}\relax }}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Functional Annotations obtained from ANNOVAR}\relax }}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9}Validation of Bayesian Network Ranking on PDX dataset}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces \textbf  {Top 30 genes from Bayesian Ranking Algorithm}\relax }}{34}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {Highest Ranked Genes from Bayesian Ranking}\relax }}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  \textbf  {5 Year Survival Curve of Patients with SPRR1A+ and SPRR1A- DLBCL.} Source : Zhang et al. (2014), Figure 2.\relax }}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces \textbf  {Circos Plot of Top 300 Ranked Genes from Bayesian Network Ranking.} In this Circos plot, the outer track indicates the top ranked genes and their positions on the chromosome. The inner track describes the type of mutation that was observed -- most mutations were non-synonymous SNVs, with a few stop-gain mutations. The innermost track shows the relative probabilities of each ranked gene.\relax }}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Comparison of Deep Learning with other Integration Methods}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Analysis of Bayesian Network}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Future Directions}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Appendixes}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Neural Network Learning}{41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Feedforward Phase}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces \textbf  {Sample Neural Networks with Labelled Nodes and Weights.}\relax }}{41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Backpropagation Phase}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces \textbf  {Backpropagation of Error Terms}\relax }}{42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Cost Function in Gradient Descent}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces \textbf  {Graphical Illustration of Gradient Descent.} Gradient descent attempts to find the gradient at which the cost function is minimised (since the cost function depends on the gradient).\relax }}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Feature Engineering}{45}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Base Information}{45}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Sequencing Biases and Errors}{46}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Calling and Mapping Qualities}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Mathematical and Statistical Tools}{49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Derivation of F1 Score}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces \textbf  {Confusion Matrix for a Binary Class Predictor.}\relax }}{49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Principal Components Analysis (PCA)}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces \textbf  {Variance Captured by First 12 Principal Components}\relax }}{50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Synthetic Minority Overrepresentation Technique (SMOTE)}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces \textbf  {Illustration of SMOTE Oversampling Algorithm.} Note the use of 2 nearest neighbours to create a new synthetic example. Figure from Chawla et al., 2002\relax }}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Bibilography}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Relevant Code}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}generate\_matrixes.py}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}train\_network.py}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}compute\_bayesian.py}{81}}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  708C9BCB12B5216BBF86DAFDD16ECE411783A10277C0CC29BFFEC5BB6F4B264B.pygtex,
  B85D79633CB32D38A6E77E93497D5E651783A10277C0CC29BFFEC5BB6F4B264B.pygtex,
  4997910BF66F97E9C5DC9311827F542B1783A10277C0CC29BFFEC5BB6F4B264B.pygtex}
